{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to KOBE KOBE is a benchmarking system that leverages Docker and Kubernetes in order to reproduce experiments of federated query processing in a collections of data sources. Overview In the SPARQL query processing community, as well as in the wider databases community, benchmark reproducibility is based on releasing datasets and query workloads. However, this paradigm breaks down for federated query processors, as these systems do not manage the data they serve to their clients but provide a data-integration abstraction over the actual query processors that are in direct contact with the data. The KOBE benchmarking engine is a system that aims to provide a generic platform to perform benchmarking and experimentation that can be reproducible in different environments. It was designed with the following objectives in mind: to allow for benchmark and experiment specifications to be reproduced in different environments and be able to produce comparable and reliable results; to ease the deployment of complex benchmarking experiments by automating the tedious tasks of initialization and execution.","title":"Home"},{"location":"#welcome-to-kobe","text":"KOBE is a benchmarking system that leverages Docker and Kubernetes in order to reproduce experiments of federated query processing in a collections of data sources.","title":"Welcome to KOBE"},{"location":"#overview","text":"In the SPARQL query processing community, as well as in the wider databases community, benchmark reproducibility is based on releasing datasets and query workloads. However, this paradigm breaks down for federated query processors, as these systems do not manage the data they serve to their clients but provide a data-integration abstraction over the actual query processors that are in direct contact with the data. The KOBE benchmarking engine is a system that aims to provide a generic platform to perform benchmarking and experimentation that can be reproducible in different environments. It was designed with the following objectives in mind: to allow for benchmark and experiment specifications to be reproduced in different environments and be able to produce comparable and reliable results; to ease the deployment of complex benchmarking experiments by automating the tedious tasks of initialization and execution.","title":"Overview"},{"location":"extend/add_dataset_server/","text":"Under Construction","title":"Add a new dataset server"},{"location":"extend/add_evaluator/","text":"Under Construction","title":"Add a new evaluator"},{"location":"extend/add_federator/","text":"Under Construction","title":"Add federator"},{"location":"extend/add_metrics/","text":"Under Construction","title":"Add new metrics"},{"location":"extend/datasettemplate/","text":"DatasetTemplate Walkthough This walkthrough illustrates the steps required from the implementor of a dataset server in order to create a DatasetTemplate specification. In KOBE, a dataset template is defined using a set of Docker images. Additional parameters include the port and the path that the container will listen for queries. Dataset templates are used in the Benchmark specifications in order to define the dataset server of the federated endpoints. Prerequisites In this walkthrough we assume that you already have already prepared a Docker image that provides the SPARQL endpoint of the dataset server (e.g., https://hub.docker.com/r/openlink/virtuoso-opensource-7). Step 1. Prepare your Docker images The first step is to provide a set of one or more Docker images that downloads the dataset, loads the data, and starts the dataset server. Even though all this functionality can be provided with a single image, we suggest to split the various tasks into three separate images. More specifically: A docker image that downloads a RDF dump from a known URL (found in the variable $DATASET_URL ) and extracts its contents in the directory /kobe/dataset/$DATASET_NAME/dump . A docker image that loads the downloaded dump (already present in the directory /kobe/dataset/$DATASET_NAME/dump ) into the dataset server. Optionally, it can back-up the contents of the database in some directory inside /kobe/dataset/$DATASET_NAME such that the loading process to be executed only once. A docker image that starts the dataset server and exposes its SPARQL endpoint. The environment variables are initialized by the Kobe operator according to the specification of the benchmark to be executed. Moreover, the shared volumes are managed through the Kobe operator too (ref. here for details about the shared storage of Kobe). In the bechmark walkthrough, we suggest that the dataset dumps should follow a specific format. Therefore, feel free to use semagrow/url-donwnloader (source code here ) as your first image. However, if you optionally want your template to support more dataset dump formats, you can implement your own url downloader. As an example, we present the images for two dataset servers (namely Virtuoso and Strabon). Regarding the Virtuoso RDF store, we use the images semagrow/url-donwnloader , semagrow/virtuoso-init (source code here ), and semagrow/virtuoso-main (source code here ). We use the shared storage of Kobe, to keep a backup of the /database directory of Virtuoso, which is used to keep all the files used by the database. The last two images are built upon openlink/virtuoso-opensource-7 . Regarding the Strabon geospatial RDF store, we use the images semagrow/url-donwnloader , semagrow/strabon-init (source code here ), and semagrow/strabon-main (source code here ). We use the shared storage of Kobe, to keep a backup of the PostGIS database (directory /var/lib/postgresql/9.4/main ) which is where the data are kept inside Strabon. The last two images are built using the docker file of KR-suite (see http://github.com/GiorgosMandi/KR-Suite-docker)`. Step 2. Prepare your YAML file Once you have prepared the docker images, creating the dataset template specification for your dataset server is a straightforward task. It should look like this (we use as an example the template for Virtuoso): apiVersion : kobe.semagrow.org/v1alpha1 kind : DatasetTemplate metadata : # Each dataset template can be uniquely identified by its name. name : virtuosotemplate spec : initContainers : # here you put the first two images (that is the images for initializing # your server in the order you want to be executed). - name : initcontainer0 image : semagrow/url-downloader - name : initcontainer1 image : semagrow/virtuoso-init containers : # here you put the last image (that is the image for serving the data) - name : maincontainer image : semagrow/virtuoso-main ports : - containerPort : 8890 # port to listen for queries port : 8890 # port to listen for queries path : /sparql # path to listen for queries The default URL for the SPARQL endpoint for Virtuoso is http://localhost:8890/sparql , hence the port and the path to listen for queries are 8890 and /sparql respectively. Examples We have already prepared several dataset template specifications to experiment with: dataset-virtuoso dataset-strabon Notice: We plan to define more dataset template specifications in the future. We place all dataset template specifications in the examples/ directory under a subdirectory with the prefix dataset-* .","title":"DatasetTemplate Walkthough"},{"location":"extend/datasettemplate/#datasettemplate-walkthough","text":"This walkthrough illustrates the steps required from the implementor of a dataset server in order to create a DatasetTemplate specification. In KOBE, a dataset template is defined using a set of Docker images. Additional parameters include the port and the path that the container will listen for queries. Dataset templates are used in the Benchmark specifications in order to define the dataset server of the federated endpoints.","title":"DatasetTemplate Walkthough"},{"location":"extend/datasettemplate/#prerequisites","text":"In this walkthrough we assume that you already have already prepared a Docker image that provides the SPARQL endpoint of the dataset server (e.g., https://hub.docker.com/r/openlink/virtuoso-opensource-7).","title":"Prerequisites"},{"location":"extend/datasettemplate/#step-1-prepare-your-docker-images","text":"The first step is to provide a set of one or more Docker images that downloads the dataset, loads the data, and starts the dataset server. Even though all this functionality can be provided with a single image, we suggest to split the various tasks into three separate images. More specifically: A docker image that downloads a RDF dump from a known URL (found in the variable $DATASET_URL ) and extracts its contents in the directory /kobe/dataset/$DATASET_NAME/dump . A docker image that loads the downloaded dump (already present in the directory /kobe/dataset/$DATASET_NAME/dump ) into the dataset server. Optionally, it can back-up the contents of the database in some directory inside /kobe/dataset/$DATASET_NAME such that the loading process to be executed only once. A docker image that starts the dataset server and exposes its SPARQL endpoint. The environment variables are initialized by the Kobe operator according to the specification of the benchmark to be executed. Moreover, the shared volumes are managed through the Kobe operator too (ref. here for details about the shared storage of Kobe). In the bechmark walkthrough, we suggest that the dataset dumps should follow a specific format. Therefore, feel free to use semagrow/url-donwnloader (source code here ) as your first image. However, if you optionally want your template to support more dataset dump formats, you can implement your own url downloader. As an example, we present the images for two dataset servers (namely Virtuoso and Strabon). Regarding the Virtuoso RDF store, we use the images semagrow/url-donwnloader , semagrow/virtuoso-init (source code here ), and semagrow/virtuoso-main (source code here ). We use the shared storage of Kobe, to keep a backup of the /database directory of Virtuoso, which is used to keep all the files used by the database. The last two images are built upon openlink/virtuoso-opensource-7 . Regarding the Strabon geospatial RDF store, we use the images semagrow/url-donwnloader , semagrow/strabon-init (source code here ), and semagrow/strabon-main (source code here ). We use the shared storage of Kobe, to keep a backup of the PostGIS database (directory /var/lib/postgresql/9.4/main ) which is where the data are kept inside Strabon. The last two images are built using the docker file of KR-suite (see http://github.com/GiorgosMandi/KR-Suite-docker)`.","title":"Step 1. Prepare your Docker images"},{"location":"extend/datasettemplate/#step-2-prepare-your-yaml-file","text":"Once you have prepared the docker images, creating the dataset template specification for your dataset server is a straightforward task. It should look like this (we use as an example the template for Virtuoso): apiVersion : kobe.semagrow.org/v1alpha1 kind : DatasetTemplate metadata : # Each dataset template can be uniquely identified by its name. name : virtuosotemplate spec : initContainers : # here you put the first two images (that is the images for initializing # your server in the order you want to be executed). - name : initcontainer0 image : semagrow/url-downloader - name : initcontainer1 image : semagrow/virtuoso-init containers : # here you put the last image (that is the image for serving the data) - name : maincontainer image : semagrow/virtuoso-main ports : - containerPort : 8890 # port to listen for queries port : 8890 # port to listen for queries path : /sparql # path to listen for queries The default URL for the SPARQL endpoint for Virtuoso is http://localhost:8890/sparql , hence the port and the path to listen for queries are 8890 and /sparql respectively.","title":"Step 2. Prepare your YAML file"},{"location":"extend/datasettemplate/#examples","text":"We have already prepared several dataset template specifications to experiment with: dataset-virtuoso dataset-strabon Notice: We plan to define more dataset template specifications in the future. We place all dataset template specifications in the examples/ directory under a subdirectory with the prefix dataset-* .","title":"Examples"},{"location":"extend/federatortemplate/","text":"FederatorTemplate Walkthough This walkthrough illustrates the steps required from the implementor of a federation engine in order to create a FederatorTemplate specification. In KOBE, a federator template is defined using a set of Docker images. Additional parameters include the port and the path that the container will listen for queries. Federator templates are used in the Experiment specifications in order to define the federation engine to be benchmarked. Prerequisites In this walkthrough we assume that you already have already prepared a Docker image that provides the SPARQL endpoint of the federation engine (e.g., https://hub.docker.com/r/semagrow/semagrow/). Moreover, you should have a piece of software that automatically constructs the configuration requred for your federator to operate (e.g., https://github.com/semagrow/sevod-scraper). Step 1. Optonal. Provide support for all evaluation metrics NOTICE: This step is optional, in a sense that it is only needed in order to support all evaluation metrics of KOBE. KOBE will be able to visualize 1) the number of retured results and 2) the total time to reciece the full result set to experimenter \"for free\" if you choose to not follow this step. One important feature of Kobe is that the experimenter can have easy access on a set of several statistics and key performance indicators for each conducted experiment. The metrics currently supported are the following: Number of returned results Total time to recieve the full result set Source selection time Query planning time Query execution time Number of sources accessed Of these evaluation metrics, only the first two can can be computed by the client side. Thus, the remaining metrics should be calculated by the federation engine itself and can be presented via a log message. However, in order for Kobe to be able to link the log message with its corresponding experiment execution and with its specific query run, the log message should contain also the following information: Experiment name Start time of the experiment Query name Run These parameters are passed from the evaluator of Kobe to the federation engine via a SPARQL comment that is attached in the query string. In order to provide full support for evaluation metrics, you should do the following: Extend your query string parser to parse the first line of the query string which follows the according regex pattern: ^\\#kobeQueryDesc Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+)$ Calculate some or all of the metrics discussed previously. Provide a log message to output the metrics according to the following regex pattern: ^I - [^ ]+ [^ ]+ - .{12} - .{20} - [^ ]+ - - Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+) - Source Selection Time: (?<source_selection_time>[0-9]+) - Compile Time: (?<compile_time>[0-9]+) - Sources: (?<sources>[0-9]+) - Execution time: (?<execution_time>[0-9]+)$ As an example, consider this pull request which contains the integration needed for KOBE in the case for the Semagrow federation engine. Step 2. Prepare your Docker images Usually, a federation engine requires some configuration files that depend on the federated endpoints (e.g., the URLs of the federated SPARQL endpoints). Thus, apart from the Docker image with the SPARQL endpoint of the federation engine, you should provide a docker image that constructs any desired configuration for each of the source endpoints, and a docker image that initializes the federator that possibly takes into account the configuration files of the source endpoints. More specifically, prepare the following images: A docker image that constructs a configuration file for a source endpoint and places it in an output directory of your choice. Assume that the source endpoint and the dataset name are available in the environment variables $DATASET_NAME and $DATASET_URL respectively, and that the dump file of the dataset is present in an input directory of your choice. A docker image that constructs a configuration file for the federation engine and places it in an output directory of your choice. Assume that all the configuration files produced ine the previous step are present in an input directory of your choice. A docker image that starts the federation engine and exposes its SPARQL endpoint. The environment variables are initialized by the Kobe operator according to the specification of the benchmark to be executed. As an example, we present the images for two federation engines (namely fedx and Semagrow). Regarding the Semagrow federation engine, we use the images semagrow/semagrow-init (source code here ), semagrow/semagrow-init-all , (source code here ), and semagrow/semagrow (see here ). The first image uses the sevod-scraper tool to create a ttl metadata file from the dump file, and the second image concatenates all metadata files of each of the source endpoints into a single metadata file. Regarding the fedx federation engine, we use the images semagrow/fedx-init (source code here ), semagrow/fedx-init-all , (source code here ), and semagrow/fedx-server (source code here ). Fedx is known for not using any dataset statistics, but it uses only a ttl file that contains only the SPARQL endpoints of the federation. The first image creates a ttl file that defines the SPARQL endpoint of each dataset and the second image concatenates all ttil files of each source endpoints into a single configuration file. Step 3. Prepare your YAML file Once you have prepared the docker images, creating the federator template specification for your dataset server is a straightforward task. It should look like this (we use as an example the template for Semagrow): apiVersion : kobe.semagrow.org/v1alpha1 kind : FederatorTemplate metadata : # Each federator template can be uniquely identified by its name. name : semagrowtemplate spec : containers : # here you put the last image (that is the image for the # SPARQL endpoint of the federation engine) - name : maincontainer image : semagrow/semagrow ports : - containerPort : 8080 # port to listen for queries port : 8080 # port to listen for queries path : /SemaGrow/sparql # path to listen for queries fedConfDir : /etc/default/semagrow # where the federator expects to find its configuration # federator configuration step 1 (for each dataset): confFromFileImage : semagrow/semagrow-init # first docker image inputDumpDir : /sevod-scraper/input # where to find the dump file for the dataset outputDumpDir : /sevod-scraper/output # where to place the configuration for the dataset # federator configuration step 2 (combination step): confImage : semagrow/semagrow-init-all # second docker image inputDir : /kobe/input # where to find all dataset configurations outputDir : /kobe/output # where to place the final (combined) configuration The default URL for the SPARQL endpoint for Virtuoso is http://localhost:8080/SemaGrow/sparql , hence the port and the path to listen for queries are 8080 and /SemaGrow/sparql respectively. The input and output directories of the images mentioned previously are configured using the parameters inputDumpDir , outputDumpDir , inputDir , outputDir . Examples We have already prepared several federator template specifications to experiment with: federator-fedx federator-semagrow federator-uno Notice: We plan to define more federator template specifications in the future. We place all federator template specifications in the examples/ directory under a subdirectory with the prefix federator-* .","title":"FederatorTemplate Walkthough"},{"location":"extend/federatortemplate/#federatortemplate-walkthough","text":"This walkthrough illustrates the steps required from the implementor of a federation engine in order to create a FederatorTemplate specification. In KOBE, a federator template is defined using a set of Docker images. Additional parameters include the port and the path that the container will listen for queries. Federator templates are used in the Experiment specifications in order to define the federation engine to be benchmarked.","title":"FederatorTemplate Walkthough"},{"location":"extend/federatortemplate/#prerequisites","text":"In this walkthrough we assume that you already have already prepared a Docker image that provides the SPARQL endpoint of the federation engine (e.g., https://hub.docker.com/r/semagrow/semagrow/). Moreover, you should have a piece of software that automatically constructs the configuration requred for your federator to operate (e.g., https://github.com/semagrow/sevod-scraper).","title":"Prerequisites"},{"location":"extend/federatortemplate/#step-1-optonal-provide-support-for-all-evaluation-metrics","text":"NOTICE: This step is optional, in a sense that it is only needed in order to support all evaluation metrics of KOBE. KOBE will be able to visualize 1) the number of retured results and 2) the total time to reciece the full result set to experimenter \"for free\" if you choose to not follow this step. One important feature of Kobe is that the experimenter can have easy access on a set of several statistics and key performance indicators for each conducted experiment. The metrics currently supported are the following: Number of returned results Total time to recieve the full result set Source selection time Query planning time Query execution time Number of sources accessed Of these evaluation metrics, only the first two can can be computed by the client side. Thus, the remaining metrics should be calculated by the federation engine itself and can be presented via a log message. However, in order for Kobe to be able to link the log message with its corresponding experiment execution and with its specific query run, the log message should contain also the following information: Experiment name Start time of the experiment Query name Run These parameters are passed from the evaluator of Kobe to the federation engine via a SPARQL comment that is attached in the query string. In order to provide full support for evaluation metrics, you should do the following: Extend your query string parser to parse the first line of the query string which follows the according regex pattern: ^\\#kobeQueryDesc Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+)$ Calculate some or all of the metrics discussed previously. Provide a log message to output the metrics according to the following regex pattern: ^I - [^ ]+ [^ ]+ - .{12} - .{20} - [^ ]+ - - Experiment: (?<experiment>[^ ]+) - Date: (?<date>[^ ]+ [^ ]+) - Query: (?<query>[^ ]+) - Run: (?<run>[0-9]+) - Source Selection Time: (?<source_selection_time>[0-9]+) - Compile Time: (?<compile_time>[0-9]+) - Sources: (?<sources>[0-9]+) - Execution time: (?<execution_time>[0-9]+)$ As an example, consider this pull request which contains the integration needed for KOBE in the case for the Semagrow federation engine.","title":"Step 1. Optonal. Provide support for all evaluation metrics"},{"location":"extend/federatortemplate/#step-2-prepare-your-docker-images","text":"Usually, a federation engine requires some configuration files that depend on the federated endpoints (e.g., the URLs of the federated SPARQL endpoints). Thus, apart from the Docker image with the SPARQL endpoint of the federation engine, you should provide a docker image that constructs any desired configuration for each of the source endpoints, and a docker image that initializes the federator that possibly takes into account the configuration files of the source endpoints. More specifically, prepare the following images: A docker image that constructs a configuration file for a source endpoint and places it in an output directory of your choice. Assume that the source endpoint and the dataset name are available in the environment variables $DATASET_NAME and $DATASET_URL respectively, and that the dump file of the dataset is present in an input directory of your choice. A docker image that constructs a configuration file for the federation engine and places it in an output directory of your choice. Assume that all the configuration files produced ine the previous step are present in an input directory of your choice. A docker image that starts the federation engine and exposes its SPARQL endpoint. The environment variables are initialized by the Kobe operator according to the specification of the benchmark to be executed. As an example, we present the images for two federation engines (namely fedx and Semagrow). Regarding the Semagrow federation engine, we use the images semagrow/semagrow-init (source code here ), semagrow/semagrow-init-all , (source code here ), and semagrow/semagrow (see here ). The first image uses the sevod-scraper tool to create a ttl metadata file from the dump file, and the second image concatenates all metadata files of each of the source endpoints into a single metadata file. Regarding the fedx federation engine, we use the images semagrow/fedx-init (source code here ), semagrow/fedx-init-all , (source code here ), and semagrow/fedx-server (source code here ). Fedx is known for not using any dataset statistics, but it uses only a ttl file that contains only the SPARQL endpoints of the federation. The first image creates a ttl file that defines the SPARQL endpoint of each dataset and the second image concatenates all ttil files of each source endpoints into a single configuration file.","title":"Step 2. Prepare your Docker images"},{"location":"extend/federatortemplate/#step-3-prepare-your-yaml-file","text":"Once you have prepared the docker images, creating the federator template specification for your dataset server is a straightforward task. It should look like this (we use as an example the template for Semagrow): apiVersion : kobe.semagrow.org/v1alpha1 kind : FederatorTemplate metadata : # Each federator template can be uniquely identified by its name. name : semagrowtemplate spec : containers : # here you put the last image (that is the image for the # SPARQL endpoint of the federation engine) - name : maincontainer image : semagrow/semagrow ports : - containerPort : 8080 # port to listen for queries port : 8080 # port to listen for queries path : /SemaGrow/sparql # path to listen for queries fedConfDir : /etc/default/semagrow # where the federator expects to find its configuration # federator configuration step 1 (for each dataset): confFromFileImage : semagrow/semagrow-init # first docker image inputDumpDir : /sevod-scraper/input # where to find the dump file for the dataset outputDumpDir : /sevod-scraper/output # where to place the configuration for the dataset # federator configuration step 2 (combination step): confImage : semagrow/semagrow-init-all # second docker image inputDir : /kobe/input # where to find all dataset configurations outputDir : /kobe/output # where to place the final (combined) configuration The default URL for the SPARQL endpoint for Virtuoso is http://localhost:8080/SemaGrow/sparql , hence the port and the path to listen for queries are 8080 and /SemaGrow/sparql respectively. The input and output directories of the images mentioned previously are configured using the parameters inputDumpDir , outputDumpDir , inputDir , outputDir .","title":"Step 3. Prepare your YAML file"},{"location":"extend/federatortemplate/#examples","text":"We have already prepared several federator template specifications to experiment with: federator-fedx federator-semagrow federator-uno Notice: We plan to define more federator template specifications in the future. We place all federator template specifications in the examples/ directory under a subdirectory with the prefix federator-* .","title":"Examples"},{"location":"extend/support_metrics/","text":"Under Construction","title":"Provide metric support"},{"location":"getting_started/install/","text":"Installation This guide illustrates the steps required to install KOBE in your system. Prerequisites Kubernetes >= 1.8.0 nfs-commons installed in the nodes of the cluster. If in debian or Ubuntu you can install it using apt-get install nfs-common Installation of the Kubernetes operator KOBE needs the Kubernetes operator to be installed in the Kubernetes cluster. To quickly install the KOBE operator in a Kubernetes cluster. You can use the kobectl script found in the bin directory: export PATH=`pwd`/bin:$PATH kobectl install operator . Alternatively, you could run the following commands: kubectl apply -f operator/deploy/crds kubectl apply -f operator/deploy/service_account.yaml kubectl apply -f operator/deploy/clusterrole.yaml kubectl apply -f operator/deploy/clusterrole_binding.yaml kubectl apply -f operator/deploy/role.yaml kubectl apply -f operator/deploy/operator.yaml You will get a confirmation message that each resource has successfully been created. This will set the operator running in your Kubernetes cluster and needs to be done only once. Installation of Networking subsystem KOBE uses istio to support network delays between the different deployments. To install istio you can run the following: kobectl install istio . Alternatively, you can consult the official installation guide or you can type the following commands. curl -L https://istio.io/downloadIstio | sh - export PATH=`pwd`/istio-1.6.0/bin:$PATH istioctl manifest apply --set profile=default Installation of the Logging subsystem To enable the evaluation metrics extraction subsystem, run kobectl install efk . or alternatively the following helm repo add elastic https://helm.elastic.co helm repo add kiwigrid https://kiwigrid.github.io helm install elastic/elasticsearch --name elasticsearch --set persistence.enabled=false --set replicas=1 --version 7.6.2 helm install elastic/kibana --name kibana --set service.type=NodePort --version 7.6.2 helm install --name fluentd -f operator/deploy/efk-config/fluentd-values.yaml kiwigrid/fluentd-elasticsearch --version 8.0.1 kubectl apply -f operator/deploy/efk-config/kobe-kibana-configuration.yaml These result in the simplest setup of an one-node Elasticsearch that does not persist data across pod recreation, a Fluentd DaemonSet and a Kibana node that exposes a NodePort . The setup can be customized by changing the configuration parameters of each helm chart. Please check the corresponding documentation of each chart for more info.","title":"Installation"},{"location":"getting_started/install/#installation","text":"This guide illustrates the steps required to install KOBE in your system.","title":"Installation"},{"location":"getting_started/install/#prerequisites","text":"Kubernetes >= 1.8.0 nfs-commons installed in the nodes of the cluster. If in debian or Ubuntu you can install it using apt-get install nfs-common","title":"Prerequisites"},{"location":"getting_started/install/#installation-of-the-kubernetes-operator","text":"KOBE needs the Kubernetes operator to be installed in the Kubernetes cluster. To quickly install the KOBE operator in a Kubernetes cluster. You can use the kobectl script found in the bin directory: export PATH=`pwd`/bin:$PATH kobectl install operator . Alternatively, you could run the following commands: kubectl apply -f operator/deploy/crds kubectl apply -f operator/deploy/service_account.yaml kubectl apply -f operator/deploy/clusterrole.yaml kubectl apply -f operator/deploy/clusterrole_binding.yaml kubectl apply -f operator/deploy/role.yaml kubectl apply -f operator/deploy/operator.yaml You will get a confirmation message that each resource has successfully been created. This will set the operator running in your Kubernetes cluster and needs to be done only once.","title":"Installation of the Kubernetes operator"},{"location":"getting_started/install/#installation-of-networking-subsystem","text":"KOBE uses istio to support network delays between the different deployments. To install istio you can run the following: kobectl install istio . Alternatively, you can consult the official installation guide or you can type the following commands. curl -L https://istio.io/downloadIstio | sh - export PATH=`pwd`/istio-1.6.0/bin:$PATH istioctl manifest apply --set profile=default","title":"Installation of Networking subsystem"},{"location":"getting_started/install/#installation-of-the-logging-subsystem","text":"To enable the evaluation metrics extraction subsystem, run kobectl install efk . or alternatively the following helm repo add elastic https://helm.elastic.co helm repo add kiwigrid https://kiwigrid.github.io helm install elastic/elasticsearch --name elasticsearch --set persistence.enabled=false --set replicas=1 --version 7.6.2 helm install elastic/kibana --name kibana --set service.type=NodePort --version 7.6.2 helm install --name fluentd -f operator/deploy/efk-config/fluentd-values.yaml kiwigrid/fluentd-elasticsearch --version 8.0.1 kubectl apply -f operator/deploy/efk-config/kobe-kibana-configuration.yaml These result in the simplest setup of an one-node Elasticsearch that does not persist data across pod recreation, a Fluentd DaemonSet and a Kibana node that exposes a NodePort . The setup can be customized by changing the configuration parameters of each helm chart. Please check the corresponding documentation of each chart for more info.","title":"Installation of the Logging subsystem"},{"location":"getting_started/run/","text":"Typical workflow The typical workflow of defining a KOBE experiment is the following. Create one DatasetTemplate for each dataset server you want to use in your benchmark. Define your Benchmark , which should contain a list of datasets and a list of queries. Create one FederatorTemplate for the federator engine you want to use in your experiment. Define an Experiment over your previously defined benchmark. Several examples of the above specifications can be found in the examples directory.","title":"Run"},{"location":"getting_started/run/#typical-workflow","text":"The typical workflow of defining a KOBE experiment is the following. Create one DatasetTemplate for each dataset server you want to use in your benchmark. Define your Benchmark , which should contain a list of datasets and a list of queries. Create one FederatorTemplate for the federator engine you want to use in your experiment. Define an Experiment over your previously defined benchmark. Several examples of the above specifications can be found in the examples directory.","title":"Typical workflow"},{"location":"getting_started/run_experiment/","text":"Perform a benchmark experiment In the following, we show the steps for deploying an experiment on a simple benchmark that comprises three queries over a Semagrow federation of two Virtuoso endpoints. You can use the kobectl script found in the bin directory for cotrolling your experiments: export PATH=`pwd`/bin:$PATH kobectl help First, apply the templates for Virtuoso and Semagrow: kobectl apply examples/dataset-virtuoso/virtuosotemplate.yaml kobectl apply examples/federator-semagrow/semagrowtemplate.yaml Then, apply the benchmark. kobectl apply examples/benchmark-toybench/toybench-simple.yaml Before running the experiment, you should verify that the datasets are loaded. Use the following command: kobectl show benchmark toybench-simple When the datasets are loaded, you should get the following output: NAME STATUS toy1 Running toy2 Running Proceed now with the execution of the experiment: kobectl apply examples/experiment-toybench/toyexp-simple.yaml As previously, you can review the state of the experiment with the following command: kobectl show experiment toyexp-simple You can now view the evaluation metrics in the Kibana dashboards. For removing all of the above, issue the following commands: kobectl delete experiment toyexp-simple kobectl delete benchmark toybench-simple kobectl delete federatortemplate semagrowtemplate kobectl delete datasettemplate virtuosotemplate For more advanced control options for KOBE, use kubectl .","title":"Perform a benchmark experiment"},{"location":"getting_started/run_experiment/#perform-a-benchmark-experiment","text":"In the following, we show the steps for deploying an experiment on a simple benchmark that comprises three queries over a Semagrow federation of two Virtuoso endpoints. You can use the kobectl script found in the bin directory for cotrolling your experiments: export PATH=`pwd`/bin:$PATH kobectl help First, apply the templates for Virtuoso and Semagrow: kobectl apply examples/dataset-virtuoso/virtuosotemplate.yaml kobectl apply examples/federator-semagrow/semagrowtemplate.yaml Then, apply the benchmark. kobectl apply examples/benchmark-toybench/toybench-simple.yaml Before running the experiment, you should verify that the datasets are loaded. Use the following command: kobectl show benchmark toybench-simple When the datasets are loaded, you should get the following output: NAME STATUS toy1 Running toy2 Running Proceed now with the execution of the experiment: kobectl apply examples/experiment-toybench/toyexp-simple.yaml As previously, you can review the state of the experiment with the following command: kobectl show experiment toyexp-simple You can now view the evaluation metrics in the Kibana dashboards. For removing all of the above, issue the following commands: kobectl delete experiment toyexp-simple kobectl delete benchmark toybench-simple kobectl delete federatortemplate semagrowtemplate kobectl delete datasettemplate virtuosotemplate For more advanced control options for KOBE, use kubectl .","title":"Perform a benchmark experiment"},{"location":"getting_started/view_results/","text":"Evaluate the results This guide illustrates how to view the results of the benchmark. Viewing the Kibana dashboards In order to view the dashboards you should have installed the Logging subsystem of KOBE. After all pods are in Running state Kibana dashboards can be accessed at http://<NODE-IP>:<NODEPORT>/app/kibana#/dashboard/ where <NODE-IP> the IP of any of the Kubernetes cluster nodes and <NODEPORT> the result of kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services kibana-kibana .","title":"Evaluate the results"},{"location":"getting_started/view_results/#evaluate-the-results","text":"This guide illustrates how to view the results of the benchmark.","title":"Evaluate the results"},{"location":"getting_started/view_results/#viewing-the-kibana-dashboards","text":"In order to view the dashboards you should have installed the Logging subsystem of KOBE. After all pods are in Running state Kibana dashboards can be accessed at http://<NODE-IP>:<NODEPORT>/app/kibana#/dashboard/ where <NODE-IP> the IP of any of the Kubernetes cluster nodes and <NODEPORT> the result of kubectl get -o jsonpath=\"{.spec.ports[0].nodePort}\" services kibana-kibana .","title":"Viewing the Kibana dashboards"},{"location":"references/api/","text":"API Docs This Document documents the types introduced by the Kobe Operator to be consumed by users. Note this document is generated from code comments. When contributing a change to this document please do so by changing the code comments. Table of Contents Benchmark BenchmarkList BenchmarkSpec Dataset DatasetEndpoint DatasetFile DatasetTemplate DatasetTemplateList Delay EphemeralDataset EphemeralDatasetList EphemeralDatasetStatus Evaluator Experiment ExperimentList ExperimentSpec ExperimentStatus Federation FederationList FederationSpec FederationStatus Federator FederatorList FederatorSpec FederatorTemplate FederatorTemplateList KobeUtil KobeUtilList NetworkConnection Query SystemDatasetSpec Benchmark Benchmark is the Schema for the benchmarks API Field Description Scheme Required metadata metav1.ObjectMeta false spec BenchmarkSpec false status BenchmarkStatus false Back to TOC BenchmarkList BenchmarkList contains a list of Benchmark Field Description Scheme Required metadata metav1.ListMeta false items [] Benchmark true Back to TOC BenchmarkSpec BenchmarkSpec defines the components for this benchmark setup Field Description Scheme Required datasets [] Dataset true queries [] Query true Back to TOC Dataset Field Description Scheme Required name string true files [] DatasetFile true systemspec * SystemDatasetSpec false templateRef string false affinity If specified, the pod's scheduling constraints * v1.Affinity false resources Resources are not allowed for ephemeral containers. Ephemeral containers use spare resources already allocated to the pod. v1.ResourceRequirements false networkTopology network delays [] NetworkConnection false federatorConnection * NetworkConnection false Back to TOC DatasetEndpoint Field Description Scheme Required host string true namespace string true port uint32 true path string true Back to TOC DatasetFile Field Description Scheme Required url string true checksum string false Back to TOC DatasetTemplate Field Description Scheme Required metadata metav1.ObjectMeta false spec SystemDatasetSpec false Back to TOC DatasetTemplateList FederatorList contains a list of Federator Field Description Scheme Required metadata metav1.ListMeta false items [] DatasetTemplate true Back to TOC Delay Field Description Scheme Required fixedDelaySec Add a fixed delay before forwarding the request. Format: 1h/1m/1s/1ms. MUST be >=1ms. *uint32 false fixedDelayMSec Add a fixed delay before forwarding the request. Format: 1h/1m/1s/1ms. MUST be >=1ms. *uint32 false percentage *uint32 false percent *uint32 false Back to TOC EphemeralDataset EphemeralDataset is the Schema for the kobedatasets API Field Description Scheme Required metadata metav1.ObjectMeta false spec Dataset false status EphemeralDatasetStatus false Back to TOC EphemeralDatasetList EphemeralDatasetList contains a list of EphemeralDataset Field Description Scheme Required metadata metav1.ListMeta false items [] EphemeralDataset true Back to TOC EphemeralDatasetStatus Field Description Scheme Required podNames []string true phase string true forceLoad bool true Back to TOC Evaluator Evaluator defines the Field Description Scheme Required image string true imagePullPolicy v1.PullPolicy false command []string false parallelism int32 false env [] v1.EnvVar false Back to TOC Experiment Experiment is the Schema for the experiments API Field Description Scheme Required metadata metav1.ObjectMeta false spec ExperimentSpec false status ExperimentStatus false Back to TOC ExperimentList ExperimentList contains a list of Experiment Field Description Scheme Required metadata metav1.ListMeta false items [] Experiment true Back to TOC ExperimentSpec ExperimentSpec defines the desired state of Experiment Field Description Scheme Required benchmark string true federatorName string true federatorSpec * FederatorSpec false federatorTemplateRef string false evaluator Evaluator true timesToRun int true restartPolicy RestartPolicy false dryRun bool true forceNewInit bool true Back to TOC ExperimentStatus ExperimentStatus defines the observed state of Experiment Field Description Scheme Required startTime Time at which this workflow started metav1.Time false completionTime Time at which this workflow completed metav1.Time false run The current iteration of the experiment It should be zero if not started yet int false phase The phase of the experiment ExperimentPhase true Back to TOC Federation Federation is the Schema for the federations API Field Description Scheme Required metadata metav1.ObjectMeta false spec FederationSpec false status FederationStatus false Back to TOC FederationList FederationList contains a list of Federation Field Description Scheme Required metadata metav1.ListMeta false items [] Federation true Back to TOC FederationSpec FederationSpec defines the desired state of Federation Field Description Scheme Required federatorName string true spec FederatorSpec true datasets [] DatasetEndpoint true topology [] NetworkConnection false initPolicy InitializationPolicy false Back to TOC FederationStatus FederationStatus defines the observed state of KobeFederation Field Description Scheme Required podNames []string true phase FederationPhase true Back to TOC Federator Federator is the Schema for the federators API Field Description Scheme Required metadata metav1.ObjectMeta false spec FederatorSpec false Back to TOC FederatorList FederatorList contains a list of Federator Field Description Scheme Required metadata metav1.ListMeta false items [] Federator true Back to TOC FederatorSpec FederatorSpec contains all necessary information for a federator Field Description Scheme Required initContainers []v1.Container false containers []v1.Container true port Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. int32 true path suffix to be added to endpoint of federator f.e ../SemaGrow/sparql string true confFromFileImage The Docker image that receives a compressed dataset and may produce configuration needed from the federator to federate this specific dataset This container will run one time for each dataset in the federation string true inputDumpDir where the above image expects the dump to be (if from dump) string true outputDumpDir where the above image will place its result config file string true confImage The Docker image that initializes the federator (equivalent to initContainers) string true inputDir string true outputDir string true fedConfDir which directory the federator needs the metadata config files in order to find them string true Back to TOC FederatorTemplate FederatorTemplate defines a federator and its components that it needs to be installed. Field Description Scheme Required metadata metav1.ObjectMeta false spec FederatorSpec false Back to TOC FederatorTemplateList Field Description Scheme Required metadata metav1.ListMeta false items [] FederatorTemplate true Back to TOC KobeUtil KobeUtil is the Schema for the kobeutils API Field Description Scheme Required metadata metav1.ObjectMeta false Back to TOC KobeUtilList KobeUtilList contains a list of KobeUtil Field Description Scheme Required metadata metav1.ListMeta false items [] KobeUtil true Back to TOC NetworkConnection Field Description Scheme Required datasetSource *string false delayInjection Delay false Back to TOC Query Query contains the query info Field Description Scheme Required name string true language string true queryString string true Back to TOC SystemDatasetSpec DatasetSpec defines the desired state of Dataset Field Description Scheme Required importContainers []v1.Container false initContainers List of initialization containers belonging to the pod. Init containers are executed in order prior to containers being started. If any init container fails, the pod is considered to have failed and is handled according to its restartPolicy. The name for an init container or normal container must be unique among all containers. Init containers may not have Lifecycle actions, Readiness probes, or Liveness probes. The resourceRequirements of an init container are taken into account during scheduling by finding the highest request/limit for each resource type, and then using the max of of that value or the sum of the normal containers. Limits are applied to init containers in a similar fashion. Init containers cannot currently be added or removed. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ []v1.Container false containers List of containers belonging to the pod. Containers cannot currently be added or removed. There must be at least one container in a Pod. Cannot be updated. []v1.Container true initPolicy Forces to download and load from dataset file InitializationPolicy true port Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. uint32 true path Path that the container will listen for queries string true Back to TOC","title":"API"},{"location":"references/api/#api-docs","text":"This Document documents the types introduced by the Kobe Operator to be consumed by users. Note this document is generated from code comments. When contributing a change to this document please do so by changing the code comments.","title":"API Docs"},{"location":"references/api/#table-of-contents","text":"Benchmark BenchmarkList BenchmarkSpec Dataset DatasetEndpoint DatasetFile DatasetTemplate DatasetTemplateList Delay EphemeralDataset EphemeralDatasetList EphemeralDatasetStatus Evaluator Experiment ExperimentList ExperimentSpec ExperimentStatus Federation FederationList FederationSpec FederationStatus Federator FederatorList FederatorSpec FederatorTemplate FederatorTemplateList KobeUtil KobeUtilList NetworkConnection Query SystemDatasetSpec","title":"Table of Contents"},{"location":"references/api/#benchmark","text":"Benchmark is the Schema for the benchmarks API Field Description Scheme Required metadata metav1.ObjectMeta false spec BenchmarkSpec false status BenchmarkStatus false Back to TOC","title":"Benchmark"},{"location":"references/api/#benchmarklist","text":"BenchmarkList contains a list of Benchmark Field Description Scheme Required metadata metav1.ListMeta false items [] Benchmark true Back to TOC","title":"BenchmarkList"},{"location":"references/api/#benchmarkspec","text":"BenchmarkSpec defines the components for this benchmark setup Field Description Scheme Required datasets [] Dataset true queries [] Query true Back to TOC","title":"BenchmarkSpec"},{"location":"references/api/#dataset","text":"Field Description Scheme Required name string true files [] DatasetFile true systemspec * SystemDatasetSpec false templateRef string false affinity If specified, the pod's scheduling constraints * v1.Affinity false resources Resources are not allowed for ephemeral containers. Ephemeral containers use spare resources already allocated to the pod. v1.ResourceRequirements false networkTopology network delays [] NetworkConnection false federatorConnection * NetworkConnection false Back to TOC","title":"Dataset"},{"location":"references/api/#datasetendpoint","text":"Field Description Scheme Required host string true namespace string true port uint32 true path string true Back to TOC","title":"DatasetEndpoint"},{"location":"references/api/#datasetfile","text":"Field Description Scheme Required url string true checksum string false Back to TOC","title":"DatasetFile"},{"location":"references/api/#datasettemplate","text":"Field Description Scheme Required metadata metav1.ObjectMeta false spec SystemDatasetSpec false Back to TOC","title":"DatasetTemplate"},{"location":"references/api/#datasettemplatelist","text":"FederatorList contains a list of Federator Field Description Scheme Required metadata metav1.ListMeta false items [] DatasetTemplate true Back to TOC","title":"DatasetTemplateList"},{"location":"references/api/#delay","text":"Field Description Scheme Required fixedDelaySec Add a fixed delay before forwarding the request. Format: 1h/1m/1s/1ms. MUST be >=1ms. *uint32 false fixedDelayMSec Add a fixed delay before forwarding the request. Format: 1h/1m/1s/1ms. MUST be >=1ms. *uint32 false percentage *uint32 false percent *uint32 false Back to TOC","title":"Delay"},{"location":"references/api/#ephemeraldataset","text":"EphemeralDataset is the Schema for the kobedatasets API Field Description Scheme Required metadata metav1.ObjectMeta false spec Dataset false status EphemeralDatasetStatus false Back to TOC","title":"EphemeralDataset"},{"location":"references/api/#ephemeraldatasetlist","text":"EphemeralDatasetList contains a list of EphemeralDataset Field Description Scheme Required metadata metav1.ListMeta false items [] EphemeralDataset true Back to TOC","title":"EphemeralDatasetList"},{"location":"references/api/#ephemeraldatasetstatus","text":"Field Description Scheme Required podNames []string true phase string true forceLoad bool true Back to TOC","title":"EphemeralDatasetStatus"},{"location":"references/api/#evaluator","text":"Evaluator defines the Field Description Scheme Required image string true imagePullPolicy v1.PullPolicy false command []string false parallelism int32 false env [] v1.EnvVar false Back to TOC","title":"Evaluator"},{"location":"references/api/#experiment","text":"Experiment is the Schema for the experiments API Field Description Scheme Required metadata metav1.ObjectMeta false spec ExperimentSpec false status ExperimentStatus false Back to TOC","title":"Experiment"},{"location":"references/api/#experimentlist","text":"ExperimentList contains a list of Experiment Field Description Scheme Required metadata metav1.ListMeta false items [] Experiment true Back to TOC","title":"ExperimentList"},{"location":"references/api/#experimentspec","text":"ExperimentSpec defines the desired state of Experiment Field Description Scheme Required benchmark string true federatorName string true federatorSpec * FederatorSpec false federatorTemplateRef string false evaluator Evaluator true timesToRun int true restartPolicy RestartPolicy false dryRun bool true forceNewInit bool true Back to TOC","title":"ExperimentSpec"},{"location":"references/api/#experimentstatus","text":"ExperimentStatus defines the observed state of Experiment Field Description Scheme Required startTime Time at which this workflow started metav1.Time false completionTime Time at which this workflow completed metav1.Time false run The current iteration of the experiment It should be zero if not started yet int false phase The phase of the experiment ExperimentPhase true Back to TOC","title":"ExperimentStatus"},{"location":"references/api/#federation","text":"Federation is the Schema for the federations API Field Description Scheme Required metadata metav1.ObjectMeta false spec FederationSpec false status FederationStatus false Back to TOC","title":"Federation"},{"location":"references/api/#federationlist","text":"FederationList contains a list of Federation Field Description Scheme Required metadata metav1.ListMeta false items [] Federation true Back to TOC","title":"FederationList"},{"location":"references/api/#federationspec","text":"FederationSpec defines the desired state of Federation Field Description Scheme Required federatorName string true spec FederatorSpec true datasets [] DatasetEndpoint true topology [] NetworkConnection false initPolicy InitializationPolicy false Back to TOC","title":"FederationSpec"},{"location":"references/api/#federationstatus","text":"FederationStatus defines the observed state of KobeFederation Field Description Scheme Required podNames []string true phase FederationPhase true Back to TOC","title":"FederationStatus"},{"location":"references/api/#federator","text":"Federator is the Schema for the federators API Field Description Scheme Required metadata metav1.ObjectMeta false spec FederatorSpec false Back to TOC","title":"Federator"},{"location":"references/api/#federatorlist","text":"FederatorList contains a list of Federator Field Description Scheme Required metadata metav1.ListMeta false items [] Federator true Back to TOC","title":"FederatorList"},{"location":"references/api/#federatorspec","text":"FederatorSpec contains all necessary information for a federator Field Description Scheme Required initContainers []v1.Container false containers []v1.Container true port Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. int32 true path suffix to be added to endpoint of federator f.e ../SemaGrow/sparql string true confFromFileImage The Docker image that receives a compressed dataset and may produce configuration needed from the federator to federate this specific dataset This container will run one time for each dataset in the federation string true inputDumpDir where the above image expects the dump to be (if from dump) string true outputDumpDir where the above image will place its result config file string true confImage The Docker image that initializes the federator (equivalent to initContainers) string true inputDir string true outputDir string true fedConfDir which directory the federator needs the metadata config files in order to find them string true Back to TOC","title":"FederatorSpec"},{"location":"references/api/#federatortemplate","text":"FederatorTemplate defines a federator and its components that it needs to be installed. Field Description Scheme Required metadata metav1.ObjectMeta false spec FederatorSpec false Back to TOC","title":"FederatorTemplate"},{"location":"references/api/#federatortemplatelist","text":"Field Description Scheme Required metadata metav1.ListMeta false items [] FederatorTemplate true Back to TOC","title":"FederatorTemplateList"},{"location":"references/api/#kobeutil","text":"KobeUtil is the Schema for the kobeutils API Field Description Scheme Required metadata metav1.ObjectMeta false Back to TOC","title":"KobeUtil"},{"location":"references/api/#kobeutillist","text":"KobeUtilList contains a list of KobeUtil Field Description Scheme Required metadata metav1.ListMeta false items [] KobeUtil true Back to TOC","title":"KobeUtilList"},{"location":"references/api/#networkconnection","text":"Field Description Scheme Required datasetSource *string false delayInjection Delay false Back to TOC","title":"NetworkConnection"},{"location":"references/api/#query","text":"Query contains the query info Field Description Scheme Required name string true language string true queryString string true Back to TOC","title":"Query"},{"location":"references/api/#systemdatasetspec","text":"DatasetSpec defines the desired state of Dataset Field Description Scheme Required importContainers []v1.Container false initContainers List of initialization containers belonging to the pod. Init containers are executed in order prior to containers being started. If any init container fails, the pod is considered to have failed and is handled according to its restartPolicy. The name for an init container or normal container must be unique among all containers. Init containers may not have Lifecycle actions, Readiness probes, or Liveness probes. The resourceRequirements of an init container are taken into account during scheduling by finding the highest request/limit for each resource type, and then using the max of of that value or the sum of the normal containers. Limits are applied to init containers in a similar fashion. Init containers cannot currently be added or removed. Cannot be updated. More info: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ []v1.Container false containers List of containers belonging to the pod. Containers cannot currently be added or removed. There must be at least one container in a Pod. Cannot be updated. []v1.Container true initPolicy Forces to download and load from dataset file InitializationPolicy true port Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. uint32 true path Path that the container will listen for queries string true Back to TOC","title":"SystemDatasetSpec"},{"location":"references/components/","text":"","title":"Components"},{"location":"references/kobectl/","text":"","title":"Command line"},{"location":"use/create_benchmark/","text":"Create a new benchmark This walkthrough illustrates the steps required from the benchmark designer in order to create a Benchmark specification. In KOBE, a benchmark comprises a collection of data sources, the latency of these data sources, and a list of query strings. Benchmarks are defined independently of the federator that is being benchmarked. Prerequisites In this walkthrough we assume that you already have already prepared the following: The dump of each RDF dataset of the benchmark. A list of query strings of the benchmark. A DatasetTemplate for each dataset server you want to use in your benchmark. Regarding the third prerequisite, we have already prepared several dataset templates to use. If you want to create your own dataset server template, check out this guide . Step 1. Prepare your dataset dumps Create a .tar.gz file for each dataset, and upload it on a known location. Place all files of the dataset into a directory, put this directory into a tar file and compress it with gzip. Even though most dataset engines support the import of several RDF formats (such as RDF/XML, turtle, etc), the most simple format is N-TRIPLES. Therefore, we suggest to store your dataset in a single .nt file. If you choose to to prepare a dump.nt file, just do the following: mkdir dataset/ mv dump.nt dataset/ tar czvf dataset.tar.gz dataset/ Finally, upload the .tar.gz file on a known location. As an example, we have uploaded the datasets for the FedBench experiment in the following location . Step 2. Prepare your YAML file A benchmark is characterized by its name and is parameterized using a list of datasets and a set of queries . A typical benchmark specification should look like this: apiVersion : kobe.semagrow.org/v1alpha1 kind : Benchmark metadata : # Each benchmark can be uniquely identified by its name. name : mybench spec : # Each benchmark consists of a set of dataset specifications. datasets : # Each dataset can be uniquely identified by its name, # and is defined with # A list of URLs that contain the dump of the dataset to download. # A specification of the dataset server to use (dataset template). - name : dataset1 files : - url : https://path/to/download/the/dataset1.tar.gz templateRef : datasettemplate # ... add more datasets ... # Each benchmark consists of a set of queries. queries : # Each query can be uniquely identified by its name, # and is defined with # The language in which the query is written (e.g., SPARQL). # The actual query string to be posed to the federator. - name : query1 language : sparql queryString : \"SELECT * WHERE ... \" # ... add more queries ... Check the following link in which we illustrate a simple example of the above specification: benchmark-toybench/toybench-simple.yaml This benchmark contains three SPARQL queries (namely tq1 , tq2 , and tq3 ), and two datasets (namely toy1 and toy2 ), both of them served by Virtuoso. Examples We have already prepared several benchmark specifications to experiment with: benchmark-fedbench benchmark-geofedbench benchmark-geographica benchmark-toybench Notice: We plan to define more benchmark specifications in the future. We place all benchmark specifications in the examples/ directory under a subdirectory with the prefix benchmark-* .","title":"Create a new benchmark"},{"location":"use/create_benchmark/#create-a-new-benchmark","text":"This walkthrough illustrates the steps required from the benchmark designer in order to create a Benchmark specification. In KOBE, a benchmark comprises a collection of data sources, the latency of these data sources, and a list of query strings. Benchmarks are defined independently of the federator that is being benchmarked.","title":"Create a new benchmark"},{"location":"use/create_benchmark/#prerequisites","text":"In this walkthrough we assume that you already have already prepared the following: The dump of each RDF dataset of the benchmark. A list of query strings of the benchmark. A DatasetTemplate for each dataset server you want to use in your benchmark. Regarding the third prerequisite, we have already prepared several dataset templates to use. If you want to create your own dataset server template, check out this guide .","title":"Prerequisites"},{"location":"use/create_benchmark/#step-1-prepare-your-dataset-dumps","text":"Create a .tar.gz file for each dataset, and upload it on a known location. Place all files of the dataset into a directory, put this directory into a tar file and compress it with gzip. Even though most dataset engines support the import of several RDF formats (such as RDF/XML, turtle, etc), the most simple format is N-TRIPLES. Therefore, we suggest to store your dataset in a single .nt file. If you choose to to prepare a dump.nt file, just do the following: mkdir dataset/ mv dump.nt dataset/ tar czvf dataset.tar.gz dataset/ Finally, upload the .tar.gz file on a known location. As an example, we have uploaded the datasets for the FedBench experiment in the following location .","title":"Step 1. Prepare your dataset dumps"},{"location":"use/create_benchmark/#step-2-prepare-your-yaml-file","text":"A benchmark is characterized by its name and is parameterized using a list of datasets and a set of queries . A typical benchmark specification should look like this: apiVersion : kobe.semagrow.org/v1alpha1 kind : Benchmark metadata : # Each benchmark can be uniquely identified by its name. name : mybench spec : # Each benchmark consists of a set of dataset specifications. datasets : # Each dataset can be uniquely identified by its name, # and is defined with # A list of URLs that contain the dump of the dataset to download. # A specification of the dataset server to use (dataset template). - name : dataset1 files : - url : https://path/to/download/the/dataset1.tar.gz templateRef : datasettemplate # ... add more datasets ... # Each benchmark consists of a set of queries. queries : # Each query can be uniquely identified by its name, # and is defined with # The language in which the query is written (e.g., SPARQL). # The actual query string to be posed to the federator. - name : query1 language : sparql queryString : \"SELECT * WHERE ... \" # ... add more queries ... Check the following link in which we illustrate a simple example of the above specification: benchmark-toybench/toybench-simple.yaml This benchmark contains three SPARQL queries (namely tq1 , tq2 , and tq3 ), and two datasets (namely toy1 and toy2 ), both of them served by Virtuoso.","title":"Step 2. Prepare your YAML file"},{"location":"use/create_benchmark/#examples","text":"We have already prepared several benchmark specifications to experiment with: benchmark-fedbench benchmark-geofedbench benchmark-geographica benchmark-toybench Notice: We plan to define more benchmark specifications in the future. We place all benchmark specifications in the examples/ directory under a subdirectory with the prefix benchmark-* .","title":"Examples"},{"location":"use/create_experiment/","text":"Create a new experiment This walkthrough illustrates the steps required from the experimenter in order to create an Experiment specification. In KOBE, an experiment is defined over a benchmark and federator. This resource provides the necessary parameters for instantiating a federation of querying systems. Prerequisites In this walkthrough we assume that you already have already prepared the following: A Benchmark for the benchmark you want to use in your experiment. A FederatorTemplate for the federation engine you want to use in your experiment. A docker image of the evaluator, which is a piece of software that will pose the queries to the federator. We have already prepared several benchmarks and federator templates to use. If you want to create your own dataset server template, check out this guide . Moreover, if you want to create your own federator template, check out this guide . Regarding the evaluator, we currently we offer the docker image semagrow/kobe-sequential-evaluator , which executes the queries of the benchmark in a sequential manner. If you want to create your own evaluater, check out this guide . Step 1. Prepare your YAML file An experiment is characterized by its name and is parameterized with a benchmark to be executed and a federator template to be used. A typical experiment specification should look like this: apiVersion : kobe.semagrow.org/v1alpha1 kind : Experiment metadata : # Each experiment can be uniquely identified by its name. name : myexperiment spec : # Specify the name of the benchmark to be executed. benchmark : mybench # Specify the name of the federation engine of the experiment. federatorName : myfederator # Specify the name of the federator template to be used. federatorTemplateRef : federatortemplate # Specify the docker image of the evaluator. evaluator : image : semagrow/kobe-sequential-evaluator # Specify the number of runs of the experiment, i.e. how many times each query # of the benchmark should be executed. timesToRun : runs # If you set this parameter to true, KOBE will only build the federation # and will not start the experiment. dryRun : false # If you set this parameter to false, KOBE will not build the federation # if it was already built in previous executions of this experiment. forceNewInit : true Check the following link in which we illustrate a simple example of the above specification: experiment-toybench/toyexp-simple.yaml In this example, we define an experiment over the toybench-simple benchmark, and we use the Semagrow federation engine. The queries of the benchmark are executed in a sequential manner, and each query of the benchmark is executed 3 times. Since toybench-simple contains the queries tq1 , tq2 , tq3 , in the example experiment the queries will be executed with the following order: tq1 , tq2 , tq3 , tq1 , tq2 , tq3 , tq1 , tq2 , tq3 . Examples We have already prepared several experiment specifications to experiment with: experiment-fedbench experiment-geofedbench experiment-geographica experiment-toybench Notice: We plan to define more experiment specifications in the future. We place all experiment specifications in the examples/ directory under a subdirectory with the prefix experiment-* .","title":"Create a new experiment"},{"location":"use/create_experiment/#create-a-new-experiment","text":"This walkthrough illustrates the steps required from the experimenter in order to create an Experiment specification. In KOBE, an experiment is defined over a benchmark and federator. This resource provides the necessary parameters for instantiating a federation of querying systems.","title":"Create a new experiment"},{"location":"use/create_experiment/#prerequisites","text":"In this walkthrough we assume that you already have already prepared the following: A Benchmark for the benchmark you want to use in your experiment. A FederatorTemplate for the federation engine you want to use in your experiment. A docker image of the evaluator, which is a piece of software that will pose the queries to the federator. We have already prepared several benchmarks and federator templates to use. If you want to create your own dataset server template, check out this guide . Moreover, if you want to create your own federator template, check out this guide . Regarding the evaluator, we currently we offer the docker image semagrow/kobe-sequential-evaluator , which executes the queries of the benchmark in a sequential manner. If you want to create your own evaluater, check out this guide .","title":"Prerequisites"},{"location":"use/create_experiment/#step-1-prepare-your-yaml-file","text":"An experiment is characterized by its name and is parameterized with a benchmark to be executed and a federator template to be used. A typical experiment specification should look like this: apiVersion : kobe.semagrow.org/v1alpha1 kind : Experiment metadata : # Each experiment can be uniquely identified by its name. name : myexperiment spec : # Specify the name of the benchmark to be executed. benchmark : mybench # Specify the name of the federation engine of the experiment. federatorName : myfederator # Specify the name of the federator template to be used. federatorTemplateRef : federatortemplate # Specify the docker image of the evaluator. evaluator : image : semagrow/kobe-sequential-evaluator # Specify the number of runs of the experiment, i.e. how many times each query # of the benchmark should be executed. timesToRun : runs # If you set this parameter to true, KOBE will only build the federation # and will not start the experiment. dryRun : false # If you set this parameter to false, KOBE will not build the federation # if it was already built in previous executions of this experiment. forceNewInit : true Check the following link in which we illustrate a simple example of the above specification: experiment-toybench/toyexp-simple.yaml In this example, we define an experiment over the toybench-simple benchmark, and we use the Semagrow federation engine. The queries of the benchmark are executed in a sequential manner, and each query of the benchmark is executed 3 times. Since toybench-simple contains the queries tq1 , tq2 , tq3 , in the example experiment the queries will be executed with the following order: tq1 , tq2 , tq3 , tq1 , tq2 , tq3 , tq1 , tq2 , tq3 .","title":"Step 1. Prepare your YAML file"},{"location":"use/create_experiment/#examples","text":"We have already prepared several experiment specifications to experiment with: experiment-fedbench experiment-geofedbench experiment-geographica experiment-toybench Notice: We plan to define more experiment specifications in the future. We place all experiment specifications in the examples/ directory under a subdirectory with the prefix experiment-* .","title":"Examples"},{"location":"use/tune_network/","text":"Tune network settings This walkthrough illustrates the steps required from the benchmark designer in order to configure the latency of the data sources of a Benchmark specification. Prerequisites In this walkthrough we assume that you already have already prepared the following: A Benchmark for the benchmark you want to use in your experiment. We have already prepared several benchmarks and federator templates to use. If you want to create your own dataset server template, check out this guide . Step 1 - Inject latency for each source endpoint KOBE allows simulating network traffic for all sources of the benchmark. For every source dataset of the benchmark, you can: inject delay in the connection between the given source endpoint and the federation engine . inject delay in the connection between the given source endpoint and another source endpoint . The reason for injecting delays between the federated sources is the fact that every SPARQL endpoint can issue a SPARQL query to every other endpoint using the SERVICE SPARQL keyword. The latency of each source can be configured using the following delay parameters . The functionality of these parameters is offered by Istio. Check this link for more information. The fixedDelaySec and fixedDelayMSec are used to indicate the amount of delay in seconds and in milliseconds. The percentage field can be used to only delay a certain percentage of requests . You can extend your benchmark specification can be extended in order to define the latency of the sources as follows: # In this example we will use two datasets, ds1 and ds2. spec : datasets : - name : ds1 # adds 1 second of delay before forwarding all responces to the federator federatorConnection : delayInjection : fixedDelaySec : 1 percentage : 100 networkTopology : # adds 2 sec of delay before forwarding the 50% of responces to the source ds1 - datasetSource : ds2 delayInjection : fixedDelaySec : 2 percentage : 50 # ... add remaining parameters for ds1 - name : ds2 # ... add remaining parameters for ds2 Check the following link in which we illustrate a simple working example with delays: benchmark-toybench/toybench-delays.yaml This benchmark contains three SPARQL queries and two datasets (namely toy1 and toy2 ). All responces from toy1 to the federator are delayed by 2 seconds and 150 milliseconds, all responces from toy2 to the federator are delayed by 2 seconds, and the 50% of the responces from toy1 to toy2 are delayed by 3 seconds.","title":"Tune network settings"},{"location":"use/tune_network/#tune-network-settings","text":"This walkthrough illustrates the steps required from the benchmark designer in order to configure the latency of the data sources of a Benchmark specification.","title":"Tune network settings"},{"location":"use/tune_network/#prerequisites","text":"In this walkthrough we assume that you already have already prepared the following: A Benchmark for the benchmark you want to use in your experiment. We have already prepared several benchmarks and federator templates to use. If you want to create your own dataset server template, check out this guide .","title":"Prerequisites"},{"location":"use/tune_network/#step-1-inject-latency-for-each-source-endpoint","text":"KOBE allows simulating network traffic for all sources of the benchmark. For every source dataset of the benchmark, you can: inject delay in the connection between the given source endpoint and the federation engine . inject delay in the connection between the given source endpoint and another source endpoint . The reason for injecting delays between the federated sources is the fact that every SPARQL endpoint can issue a SPARQL query to every other endpoint using the SERVICE SPARQL keyword. The latency of each source can be configured using the following delay parameters . The functionality of these parameters is offered by Istio. Check this link for more information. The fixedDelaySec and fixedDelayMSec are used to indicate the amount of delay in seconds and in milliseconds. The percentage field can be used to only delay a certain percentage of requests . You can extend your benchmark specification can be extended in order to define the latency of the sources as follows: # In this example we will use two datasets, ds1 and ds2. spec : datasets : - name : ds1 # adds 1 second of delay before forwarding all responces to the federator federatorConnection : delayInjection : fixedDelaySec : 1 percentage : 100 networkTopology : # adds 2 sec of delay before forwarding the 50% of responces to the source ds1 - datasetSource : ds2 delayInjection : fixedDelaySec : 2 percentage : 50 # ... add remaining parameters for ds1 - name : ds2 # ... add remaining parameters for ds2 Check the following link in which we illustrate a simple working example with delays: benchmark-toybench/toybench-delays.yaml This benchmark contains three SPARQL queries and two datasets (namely toy1 and toy2 ). All responces from toy1 to the federator are delayed by 2 seconds and 150 milliseconds, all responces from toy2 to the federator are delayed by 2 seconds, and the 50% of the responces from toy1 to toy2 are delayed by 3 seconds.","title":"Step 1 - Inject latency for each source endpoint"}]}